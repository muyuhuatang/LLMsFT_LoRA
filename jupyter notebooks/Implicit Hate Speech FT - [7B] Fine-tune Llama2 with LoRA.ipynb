{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43adad9b",
   "metadata": {},
   "source": [
    "# Fine-tune Llama2 with LoRA for QA\n",
    "\n",
    "\n",
    "reference code: https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf7d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('error', category=DeprecationWarning)\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3cb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90400e99",
   "metadata": {},
   "source": [
    "Error [NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported] resolving:\n",
    "\n",
    "1. pip install -U datasets\n",
    "2. pip install fsspec==2023.9.2\n",
    "3. restart the kernel of this jupyter notebook\n",
    "\n",
    "https://stackoverflow.com/questions/77433096/notimplementederror-loading-a-dataset-cached-in-a-localfilesystem-is-not-suppor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72bad02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>gpt_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and i will point it out here when u call whit...</td>\n",
       "      <td>the post is implicitly hateful because it comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiwhites expose their hatred of white kids ...</td>\n",
       "      <td>The post implies that any opposition to the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interior sec y salazar pushing for hispanic n...</td>\n",
       "      <td>the post is implicitly hateful because it sugg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>national socialism is race based which means ...</td>\n",
       "      <td>The post expresses support for National Social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sez the cop hater in chief the terrorist said...</td>\n",
       "      <td>the post is implicitly hateful because it dehu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post   \n",
       "0   and i will point it out here when u call whit...  \\\n",
       "1   antiwhites expose their hatred of white kids ...   \n",
       "2   interior sec y salazar pushing for hispanic n...   \n",
       "3   national socialism is race based which means ...   \n",
       "4   sez the cop hater in chief the terrorist said...   \n",
       "\n",
       "                                          gpt_answer  \n",
       "0  the post is implicitly hateful because it comp...  \n",
       "1  The post implies that any opposition to the co...  \n",
       "2  the post is implicitly hateful because it sugg...  \n",
       "3  The post expresses support for National Social...  \n",
       "4  the post is implicitly hateful because it dehu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the local dataset\n",
    "df_corpus = pd.read_csv('dataset/fine-tune-corpus-NLE.csv')\n",
    "df_corpus.head()\n",
    "\n",
    "## Extra skill, if data corpus is more than 200k instances, using the below codes will help to improve the loading efficiency giganticly\n",
    "## for json filesm transfer it into the arrow format using save_to_disk\n",
    "# dataset.save_to_disk(save_path)\n",
    "# dataset = load_from_disk(save_path)\n",
    "## In the map function\n",
    "# dataset = dataset.map(map_fn, num_proc=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859b377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['post', 'gpt_answer'],\n",
       "    num_rows: 97\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus = pd.read_csv('dataset/fine-tune-corpus-NLE.csv')\n",
    "df_corpus = df_corpus[3:].reset_index(drop=True)\n",
    "dataset = Dataset.from_pandas(df_corpus)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f7593c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b5c77457204ad5bf05ca2759f8f234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/fanhuan/cache/llama-2-7b-hf and are newly initialized: ['model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# # Dataset\n",
    "# data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"/home/fanhuan/cache/llama-2-7b-hf\"\n",
    "refined_model = \"/home/fanhuan/cache/llama-2-7b-hf-TF-LoRA-IHS\"\n",
    "cache_dir = \"/data/fanhuan/cache/temp/7b-lora-IHS\"\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "#     device_map={\"\": 7}\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23757e",
   "metadata": {},
   "source": [
    "How to use the neptune features in Transformers:\n",
    "\n",
    "https://docs.neptune.ai/integrations/transformers/#__tabbed_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b709157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2a7bc348164a4ebffc1387efa0115a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.474700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.318900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.7213834228515625, metrics={'train_runtime': 43.0178, 'train_samples_per_second': 11.274, 'train_steps_per_second': 2.906, 'total_flos': 1105959549247488.0, 'train_loss': 0.7213834228515625, 'epoch': 5.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.integrations import NeptuneCallback\n",
    "# import neptune\n",
    "\n",
    "# run = neptune.init_run(\n",
    "#     project=\"fhuang181/LoRA\",\n",
    "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZmI2ZTA2OC00ZGIxLTQ2NDktYTU4YS1jOWUyNWIwYmU3YWUifQ==\",\n",
    "# )\n",
    "\n",
    "# neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=cache_dir,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    # very important setting for keep the disk space enough for further training\n",
    "    save_total_limit = 1,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # use the report_to parameter to avoid error in neptune stuff\n",
    "    # report_to=\"none\"\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Adding the format of SFTTrainer based on the columns of dataset loaded\n",
    "# https://huggingface.co/docs/trl/sft_trainer\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['post'])):\n",
    "        text = f\"### Question: Given the short text of: {example['post'][i]}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\\n ### Answer:\\n{example['gpt_answer'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"### Answer:\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=llama_tokenizer)\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_parameters,\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    "    ## if we need to use customized training corpus, it is better to use formatting_func and data_collator \n",
    "    # dataset_text_field=\"text\",\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "#     callbacks=[neptune_callback]\n",
    ")\n",
    "\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6407b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/fanhuan/cache/llama-2-7b-hf-TF-LoRA-IHS/tokenizer_config.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-hf-TF-LoRA-IHS/special_tokens_map.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-hf-TF-LoRA-IHS/tokenizer.model',\n",
       " '/home/fanhuan/cache/llama-2-7b-hf-TF-LoRA-IHS/added_tokens.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-hf-TF-LoRA-IHS/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(refined_model)\n",
    "fine_tuning.tokenizer.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce31ab69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST]\n",
      " nobody:\n",
      "[INST] How do I use the OpenAI API? [/INST] nobody:\n",
      "[INST] How do I use the OpenAI API? [/INST] nobody: The OpenAI API is a powerful tool that allows developers to integrate the capabilities of OpenAI's artificial intelligence models into their own applications. It provides access to a wide range of models, including GPT-3, DALL-E 2, and CLIP, and allows developers to train their own models on the OpenAI API platform. To use the OpenAI API, you will need to create an account and obtain an API key. Once you have an API key, you can start making requests to the OpenAI API platform and integrating the capabilities of OpenAI's models into your own applications.\n",
      "[INST] How do I use the OpenAI API? [/INST] nobody\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=fine_tuning.model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1bca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST]\n",
      " nobody:\n",
      "[INST] How do I use the OpenAI API? [/INST] nobody:\n",
      "[INST] How do I use the OpenAI API? [/INST] nobody: The OpenAI API is a powerful tool that allows developers to integrate the capabilities of OpenAI's artificial intelligence models into their own applications. It provides access to a wide range of models, including GPT-3, DALL-E 2, and CLIP, and allows developers to train their own models on the OpenAI API platform. To use the OpenAI API, you will need to create an account and obtain an API key. Once you have an API key, you can start making requests to the OpenAI API platform and integrating the capabilities of OpenAI's models into your own applications.\n",
      "[INST] How do I use the OpenAI API? [/INST] nobody\n"
     ]
    }
   ],
   "source": [
    "# Original model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b219263",
   "metadata": {},
   "source": [
    "### Due to the issue of: The model 'PeftModelForCausalLM' is not supported for text-generation.\n",
    "\n",
    "There is no specific difference for the pipeline code in two model settings, it is due to the error of PeftModelForCausalLM is not supported yet in Transformers pipelines.\n",
    "\n",
    "According to (https://huggingface.co/bertin-project/bertin-alpaca-lora-7b/discussions/1), it is better to simply use the generate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408dc538",
   "metadata": {},
   "source": [
    "## Controlled generation, via generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccc4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pa≈∫dziernik 23, 2021\n",
      "The OpenAI API is an API that provides access to machine learning models developed by OpenAI, a leading AI research organization. The API provides pre-trained models and the ability to train new models using a variety of tasks and datasets, allowing developers to build AI-powered applications and services.\n",
      "To use the OpenAI API, you will need to create an OpenAI API Key, which is a unique identifier that allows access to the API. You can create your OpenAI API Key through the OpenAI website or through the OpenAI API Documentation.\n",
      "Once you have your OpenAI API Key, you can start using the OpenAI API. This includes training new models, accessing pre-trained models, and developing AI-powered applications and services. To get started, check out the OpenAI API Documentation, which provides detailed instructions on how to use the API.\n",
      "The OpenAI API is a powerful tool for developing AI-powered applications and services. With its pre-trained models and the ability to train new models, it provides a versatile and easy-to-use solution for building AI-powered applications and services.\n",
      "In conclusion, the OpenAI API is a powerful tool that provides access to machine learning models developed by OpenAI, a leading AI research organization. It allows developers to create AI-powered applications and services using pre-trained models and the ability to train new models. With its versatile and easy-to-use solution, the OpenAI API is a great choice for developers looking to incorporate AI into their applications and services.\n",
      "OpenAI API Exercises\n",
      "Introduction to OpenAI API exercises\n",
      "The OpenAI API provides access to a range of machine learning models developed by OpenAI, a leading AI research organization. To use these models, developers can make API requests that return the raw representation of the model or the predictions made by the model. To make these API requests, developers need to have an OpenAI API key and understand the structure of the API.\n",
      "In this exercises, we will introduce you to the basics of using the OpenAI API, including creating an OpenAI API key, making API requests, and interpreting the responses. We will also introduce you to some of the machine learning models available on the OpenAI API, and provide you with sample API\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - before fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = base_model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f28340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " hopefully, it is accessible. Some APIs require specific technology or programming language expertise, which may not be available to all developers.\n",
      "The most important step is to choose the right API for your project. There are different OpenAI APIs for different tasks, so it's important to select the one that best fits your needs.\n",
      "Next, you'll need to create an OpenAI account and obtain an API key.\n",
      "After you have obtained an API key, you can start making API requests to access the information you need.\n",
      "The OpenAI API is an exciting development in the field of artificial intelligence and machine learning. By providing access to cutting-edge models and technology, it has made it easier for developers to build new and innovative applications. With the help of an API key, developers can now incorporate the power of OpenAI's technology into their own projects, increasing the potential for innovation and growth in the field of AI. However, it's important to consider the costs and limitations of using an API before integrating it into a project. By selecting the right API for your project and following the steps outlined in this guide, you can leverage the power of OpenAI's technology and drive innovation in your own projects.\n",
      "Is there any special instructions or requirements for using the OpenAI API?\n",
      "The OpenAI API has specific instructions and requirements for using the API, including any specific technology or programming language knowledge required to use it. It's important to carefully review and follow these instructions if you're planning to use the API in your project. Additionally, you may need to obtain an API key to access the information you need from the API. The key will be tied to your OpenAI account and may require some setup or configuration. It's important to keep your key secure and follow the API's terms of service.\n",
      "How can I test the OpenAI API before integrating it into my project?\n",
      "The best way to test the OpenAI API before integrating it into your project is to start with a small pilot project. This will allow you to get familiar with the API and any specific requirements or instructions that may be required. You can also read reviews and feedback from other developers who have used the API to get a better understanding of its capabilities and limitations. Additionally, it's a good idea to reach out to the API provider for assistance and guidance on how to use the API effectively.\n",
      "What are\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - after fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = fine_tuning.model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03899d7a",
   "metadata": {},
   "source": [
    "# Fine-grained performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd0afc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>human_answer</th>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <th>llama2-13b-chat-FT-LoRA</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <th>llama2-7b-FT-LoRA</th>\n",
       "      <th>llama2-7b-FT</th>\n",
       "      <th>llama2-13b-chat-FT</th>\n",
       "      <th>llama2-13b-chat-FT-LoRA-IHS</th>\n",
       "      <th>llama2-7b-FT-LoRA-IHS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and i will point it out here when u call whit...</td>\n",
       "      <td>Blacks blame whites</td>\n",
       "      <td>\\n\\nSo for example, if you would answer Yes, p...</td>\n",
       "      <td>\\n\\nYes: Explain that this text is an example ...</td>\n",
       "      <td>\\n\\nAnswer: I do not think implicit hate speec...</td>\n",
       "      <td>\\nThank you, I greatly appreciate any and all ...</td>\n",
       "      <td>[/INST] You have any further or other question...</td>\n",
       "      <td>,. -. a-,,\\n as of and to is the, and-. of. th...</td>\n",
       "      <td>\\nThis post implies that calling someone a whi...</td>\n",
       "      <td>\\nThis statement implies that anyone who disag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiwhites expose their hatred of white kids ...</td>\n",
       "      <td>Anti-whites hate white people</td>\n",
       "      <td>\\n\\nOf course, I won't be offended by your ans...</td>\n",
       "      <td>\\n\\nNote that if you do not see any implicit h...</td>\n",
       "      <td>\\n### No \\nThis text was not containing any ha...</td>\n",
       "      <td>\\n\\n\\n\\nHere is the text: \\n\\n\\nHey buddy what...</td>\n",
       "      <td>Can you explain? - - - - - - - - - - - - - - ...</td>\n",
       "      <td>and for the,en of ( as, is the\":, and en and o...</td>\n",
       "      <td>The context of the post is not given, but it's...</td>\n",
       "      <td>\\nThe post implies that white people are super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interior sec y salazar pushing for hispanic n...</td>\n",
       "      <td>Minorities are taking over society</td>\n",
       "      <td>So that it's clear to others what was the reas...</td>\n",
       "      <td>\\n\\nHere is the short text: and i will point i...</td>\n",
       "      <td>\\n- It's clear this text is containing implici...</td>\n",
       "      <td>Thank you.\\n\\nI hope that isn't too long üëåüèºüíñ\\n...</td>\n",
       "      <td>It Is also deliberately deliberately misreali...</td>\n",
       "      <td>, that a which as,, a,,\\n./3. that the and of,...</td>\n",
       "      <td>This post is implicitly hateful because it use...</td>\n",
       "      <td>\\nthe post is implicitly hateful because it im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post   \n",
       "0   and i will point it out here when u call whit...  \\\n",
       "1   antiwhites expose their hatred of white kids ...   \n",
       "2   interior sec y salazar pushing for hispanic n...   \n",
       "\n",
       "                         human_answer   \n",
       "0                 Blacks blame whites  \\\n",
       "1       Anti-whites hate white people   \n",
       "2  Minorities are taking over society   \n",
       "\n",
       "                                     llama2-13b-chat   \n",
       "0  \\n\\nSo for example, if you would answer Yes, p...  \\\n",
       "1  \\n\\nOf course, I won't be offended by your ans...   \n",
       "2  So that it's clear to others what was the reas...   \n",
       "\n",
       "                             llama2-13b-chat-FT-LoRA   \n",
       "0  \\n\\nYes: Explain that this text is an example ...  \\\n",
       "1  \\n\\nNote that if you do not see any implicit h...   \n",
       "2  \\n\\nHere is the short text: and i will point i...   \n",
       "\n",
       "                                           llama2-7b   \n",
       "0  \\n\\nAnswer: I do not think implicit hate speec...  \\\n",
       "1  \\n### No \\nThis text was not containing any ha...   \n",
       "2  \\n- It's clear this text is containing implici...   \n",
       "\n",
       "                                   llama2-7b-FT-LoRA   \n",
       "0  \\nThank you, I greatly appreciate any and all ...  \\\n",
       "1  \\n\\n\\n\\nHere is the text: \\n\\n\\nHey buddy what...   \n",
       "2  Thank you.\\n\\nI hope that isn't too long üëåüèºüíñ\\n...   \n",
       "\n",
       "                                        llama2-7b-FT   \n",
       "0  [/INST] You have any further or other question...  \\\n",
       "1   Can you explain? - - - - - - - - - - - - - - ...   \n",
       "2   It Is also deliberately deliberately misreali...   \n",
       "\n",
       "                                  llama2-13b-chat-FT   \n",
       "0  ,. -. a-,,\\n as of and to is the, and-. of. th...  \\\n",
       "1  and for the,en of ( as, is the\":, and en and o...   \n",
       "2  , that a which as,, a,,\\n./3. that the and of,...   \n",
       "\n",
       "                         llama2-13b-chat-FT-LoRA-IHS   \n",
       "0  \\nThis post implies that calling someone a whi...  \\\n",
       "1  The context of the post is not given, but it's...   \n",
       "2  This post is implicitly hateful because it use...   \n",
       "\n",
       "                               llama2-7b-FT-LoRA-IHS  \n",
       "0  \\nThis statement implies that anyone who disag...  \n",
       "1  \\nThe post implies that white people are super...  \n",
       "2  \\nthe post is implicitly hateful because it im...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/implicit-hate-speech.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79dad217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:44<00:00, 34.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "#     # original model \n",
    "#     model = base_model\n",
    "#     tokenizer = llama_tokenizer\n",
    "    \n",
    "#     tweet = df.loc[0,'post']\n",
    "\n",
    "#     text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "#     inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "#     outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "#                                                pad_token_id=tokenizer.eos_token_id,\n",
    "#                                                eos_token_id=tokenizer.eos_token_id,\n",
    "#                                                return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "#     input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "#     generated_tokens = outputs.sequences[:, input_length:]\n",
    "#     temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "#     df.loc[i, 'llama2-13b-chat'] = temp_output\n",
    "    \n",
    "    # fine-tuned model, using LoRA\n",
    "    model = fine_tuning.model\n",
    "    tokenizer = llama_tokenizer\n",
    "    \n",
    "    tweet = df.loc[0,'post']\n",
    "\n",
    "    text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                               pad_token_id=tokenizer.eos_token_id,\n",
    "                                               eos_token_id=tokenizer.eos_token_id,\n",
    "                                               return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "    df.loc[i, 'llama2-7b-FT-LoRA-IHS'] = temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e5a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/implicit-hate-speech.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce5d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
