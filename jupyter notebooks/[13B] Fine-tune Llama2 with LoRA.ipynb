{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43adad9b",
   "metadata": {},
   "source": [
    "# Fine-tune Llama2 with LoRA for QA\n",
    "\n",
    "\n",
    "reference code: https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf7d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('error', category=DeprecationWarning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3cb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90400e99",
   "metadata": {},
   "source": [
    "Error [NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported] resolving:\n",
    "\n",
    "1. pip install -U datasets\n",
    "2. pip install fsspec==2023.9.2\n",
    "3. restart the kernel of this jupyter notebook\n",
    "\n",
    "https://stackoverflow.com/questions/77433096/notimplementederror-loading-a-dataset-cached-in-a-localfilesystem-is-not-suppor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f7593c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c971dc1a008841d791b0fa50e0960295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/fanhuan/cache/llama-2-13b-chat-hf and are newly initialized: ['model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"/home/fanhuan/cache/llama-2-13b-chat-hf\"\n",
    "refined_model = \"/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA\"\n",
    "cache_dir = \"/data/fanhuan/cache/temp/13b-lora\"\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "#     device_map={\"\": 7}\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23757e",
   "metadata": {},
   "source": [
    "How to use the neptune features in Transformers:\n",
    "\n",
    "https://docs.neptune.ai/integrations/transformers/#__tabbed_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b709157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 06:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.431600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.262870246887207, metrics={'train_runtime': 415.0121, 'train_samples_per_second': 2.41, 'train_steps_per_second': 0.602, 'total_flos': 1.679743262306304e+16, 'train_loss': 1.262870246887207, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.integrations import NeptuneCallback\n",
    "# import neptune\n",
    "\n",
    "# run = neptune.init_run(\n",
    "#     project=\"fhuang181/LoRA\",\n",
    "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZmI2ZTA2OC00ZGIxLTQ2NDktYTU4YS1jOWUyNWIwYmU3YWUifQ==\",\n",
    "# )\n",
    "\n",
    "# neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=cache_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    # very important setting for keep the disk space enough for further training\n",
    "    save_total_limit = 1,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # use the report_to parameter to avoid error in neptune stuff\n",
    "    # report_to=\"none\"\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    "#     callbacks=[neptune_callback]\n",
    ")\n",
    "\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d6407b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA/tokenizer_config.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA/special_tokens_map.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA/tokenizer.model',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA/added_tokens.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(refined_model)\n",
    "fine_tuning.tokenizer.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce31ab69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST] The OpenAI API is a powerful tool that allows developers to access a wide range of AI models and capabilities. Here are some steps to help you get started with using the OpenAI API:\n",
      "\n",
      "1. Sign up for an OpenAI account: To use the OpenAI API, you need to sign up for an OpenAI account. You can sign up for free on the OpenAI website.\n",
      "2. Create a new project: Once you have an OpenAI account, you can create a new project. This will allow you to access the OpenAI API and start building your AI application.\n",
      "3. Choose an AI model: The OpenAI API provides access to a wide range of AI models, including language models, computer vision models, and more. You can choose the model that best fits your needs.\n",
      "4. Use the OpenAI API: Once you have chosen an\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=fine_tuning.model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb1bca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST] The OpenAI API is a powerful tool that allows developers to access a wide range of AI models and capabilities. Here are some steps to help you get started with the OpenAI API:\n",
      "\n",
      "1. Sign up for an OpenAI account: To use the OpenAI API, you need to sign up for an OpenAI account. You can sign up for free on the OpenAI website.\n",
      "2. Create a new project: Once you have an OpenAI account, you can create a new project. This will allow you to access the OpenAI API and start building your AI application.\n",
      "3. Choose an AI model: The OpenAI API provides access to a wide range of AI models, including language models, computer vision models, and more. You can choose the model that best fits your needs.\n",
      "4. Use the OpenAI API: Once you have chosen an A\n"
     ]
    }
   ],
   "source": [
    "# Original model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b219263",
   "metadata": {},
   "source": [
    "### Due to the issue of: The model 'PeftModelForCausalLM' is not supported for text-generation.\n",
    "\n",
    "There is no specific difference for the pipeline code in two model settings, it is due to the error of PeftModelForCausalLM is not supported yet in Transformers pipelines.\n",
    "\n",
    "According to (https://huggingface.co/bertin-project/bertin-alpaca-lora-7b/discussions/1), it is better to simply use the generate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408dc538",
   "metadata": {},
   "source": [
    "## Controlled generation, via generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ccc4920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[closed]\\n\\nThe OpenAI API appears to be unstable, but that doesn\\'t seem to be the case. I\\'ve been struggling with the OpenAI API for a few weeks and I can\\'t figure out how to use it properly. I\\'ve tried using the code from the OpenAI website but none of it works for me.\\n\\nThis is my first time using a machine learning API so I\\'m open to suggestions.\\n\\nThe error I\\'m seeing is \"Not supported\", so I\\'m starting to assume it may be because I don\\'t have the right permissions.\\n\\nAny advice would be helpful.\\n\\n[What I\\'ve tried]\\n\\nI\\'ve tried a few different things so far that I found on the OpenAI website but none of it works for me. I have a feeling it may be because I don\\'t have the right permissions, but I\\'m not sure.\\n\\nThe error I\\'m seeing is \"Not supported\", and some tutorials I found online have this error as well. If you have experience working with OpenAI, please let me know what I\\'m doing wrong.\\n\\nI\\'ve tried using the Jupyter Notebook as well, and it does not work.\\n\\nPlease let me know what might be going wrong and any steps I could take to troubleshoot.</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Text - before fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = base_model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5f28340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "\n",
      "### OpenAI API endpoints\n",
      "\n",
      " - [GET] `https://api.openai.com/v1`\n",
      " - [POST] `https://api.openai.com/v1/{operation}`\n",
      "\n",
      "### Authentication\n",
      "\n",
      " - API key: `Authorization: Bearer {API_KEY}`\n",
      " - Token: `Authorization: Bearer {TOKEN}`\n",
      " \n",
      "### Operations\n",
      "\n",
      " - [GET] `https://api.openai.com/v1/models`\n",
      " - [GET] `https://api.openai.com/v1/models/{id}`\n",
      " - [POST] `https://api.openai.com/v1/models/{id}/parameters`\n",
      "```\n",
      "### Request body\n",
      "\n",
      " - [POST] `https://api.openai.com/v1/models/{id}/parameters`\n",
      " {\n",
      "  \"description\": \"My AI model\",\n",
      "  \"model\": {\n",
      "    \"model_name\": \"My model\",\n",
      "    \"model_version\": 1,\n",
      "    \"description\": \"My AI model\",\n",
      "    \"class\": \"ai.Model\",\n",
      "    \"type\": \"My model\",\n",
      "    \"labels\": [\n",
      "      {\n",
      "        \"label\": \"My label\",\n",
      "        \"description\": \"My label\",\n",
      "        \"values\": [\n",
      "          [\n",
      "            \"My label value 1\",\n",
      "            \"My label value 2\"\n",
      "          ]\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n",
      "This endpoint allows me to create, edit, delete models, including parameters. For more information, read [my previous article] or check the detailed [OpenAI API documentation]. OpenAI API endpoints are described in [this] and the main documentation [there]. You can get the API key or token by registering in [the OpenAI account] and generating [the credentials]\n",
      "```\n",
      "### Response\n",
      "\n",
      "This response endpoint returns the request parameters and status code. To test the authentication and request parameters, use `cURL`. Here is an example:\n",
      "```\n",
      "curl -X POST https://api.openai.com/v1/models \\ \n",
      " -H 'Authorization: Bearer {API_KEY}' -H 'Content-Type: application/json' -d '{}'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - after fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = fine_tuning.model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03899d7a",
   "metadata": {},
   "source": [
    "# Fine-grained performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd0afc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>human_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and i will point it out here when u call whit...</td>\n",
       "      <td>Blacks blame whites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiwhites expose their hatred of white kids ...</td>\n",
       "      <td>Anti-whites hate white people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interior sec y salazar pushing for hispanic n...</td>\n",
       "      <td>Minorities are taking over society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>national socialism is race based which means ...</td>\n",
       "      <td>Minorities are subpar races</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sez the cop hater in chief the terrorist said...</td>\n",
       "      <td>Blacks do the bidding of jews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>why should the white race not enjoy a homeland...</td>\n",
       "      <td>White people are being replaced by minorities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>wow robe  sounds like a 10 year old stamping h...</td>\n",
       "      <td>Robert is insecure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>yea we were better offwith the floppy eared ga...</td>\n",
       "      <td>the group is wasting money.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>you go guys white power</td>\n",
       "      <td>White people are powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>you re lying brazenly you accuse us of hate it...</td>\n",
       "      <td>Muslims are terrorists</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post   \n",
       "0    and i will point it out here when u call whit...  \\\n",
       "1    antiwhites expose their hatred of white kids ...   \n",
       "2    interior sec y salazar pushing for hispanic n...   \n",
       "3    national socialism is race based which means ...   \n",
       "4    sez the cop hater in chief the terrorist said...   \n",
       "..                                                ...   \n",
       "95  why should the white race not enjoy a homeland...   \n",
       "96  wow robe  sounds like a 10 year old stamping h...   \n",
       "97  yea we were better offwith the floppy eared ga...   \n",
       "98                            you go guys white power   \n",
       "99  you re lying brazenly you accuse us of hate it...   \n",
       "\n",
       "                                     human_answer  \n",
       "0                             Blacks blame whites  \n",
       "1                   Anti-whites hate white people  \n",
       "2              Minorities are taking over society  \n",
       "3                     Minorities are subpar races  \n",
       "4                   Blacks do the bidding of jews  \n",
       "..                                            ...  \n",
       "95  White people are being replaced by minorities  \n",
       "96                            Robert is insecure.  \n",
       "97                    the group is wasting money.  \n",
       "98                     White people are powerful.  \n",
       "99                         Muslims are terrorists  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/implicit-hate-speech.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79dad217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 3/3 [01:07<00:00, 22.49s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "    # original model \n",
    "    model = base_model\n",
    "    tokenizer = llama_tokenizer\n",
    "    \n",
    "    tweet = df.loc[0,'post']\n",
    "\n",
    "    text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                               pad_token_id=tokenizer.eos_token_id,\n",
    "                                               eos_token_id=tokenizer.eos_token_id,\n",
    "                                               return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "    df.loc[i, 'llama2-13b-chat'] = temp_output\n",
    "    \n",
    "    # fine-tuned model, using LoRA\n",
    "    model = fine_tuning.model\n",
    "    tokenizer = llama_tokenizer\n",
    "    \n",
    "    tweet = df.loc[0,'post']\n",
    "\n",
    "    text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                               pad_token_id=tokenizer.eos_token_id,\n",
    "                                               eos_token_id=tokenizer.eos_token_id,\n",
    "                                               return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "    df.loc[i, 'llama2-13b-chat-FT-LoRA'] = temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1e5a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/implicit-hate-speech.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
