{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43adad9b",
   "metadata": {},
   "source": [
    "# Fine-tune Llama2 with LoRA for QA\n",
    "\n",
    "\n",
    "reference code: https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf7d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('error', category=DeprecationWarning)\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3cb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90400e99",
   "metadata": {},
   "source": [
    "Error [NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported] resolving:\n",
    "\n",
    "1. pip install -U datasets\n",
    "2. pip install fsspec==2023.9.2\n",
    "3. restart the kernel of this jupyter notebook\n",
    "\n",
    "https://stackoverflow.com/questions/77433096/notimplementederror-loading-a-dataset-cached-in-a-localfilesystem-is-not-suppor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72bad02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>gpt_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and i will point it out here when u call whit...</td>\n",
       "      <td>the post is implicitly hateful because it comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiwhites expose their hatred of white kids ...</td>\n",
       "      <td>The post implies that any opposition to the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interior sec y salazar pushing for hispanic n...</td>\n",
       "      <td>the post is implicitly hateful because it sugg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>national socialism is race based which means ...</td>\n",
       "      <td>The post expresses support for National Social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sez the cop hater in chief the terrorist said...</td>\n",
       "      <td>the post is implicitly hateful because it dehu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post   \n",
       "0   and i will point it out here when u call whit...  \\\n",
       "1   antiwhites expose their hatred of white kids ...   \n",
       "2   interior sec y salazar pushing for hispanic n...   \n",
       "3   national socialism is race based which means ...   \n",
       "4   sez the cop hater in chief the terrorist said...   \n",
       "\n",
       "                                          gpt_answer  \n",
       "0  the post is implicitly hateful because it comp...  \n",
       "1  The post implies that any opposition to the co...  \n",
       "2  the post is implicitly hateful because it sugg...  \n",
       "3  The post expresses support for National Social...  \n",
       "4  the post is implicitly hateful because it dehu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the local dataset\n",
    "df_corpus = pd.read_csv('dataset/fine-tune-corpus-NLE.csv')\n",
    "df_corpus.head()\n",
    "\n",
    "## Extra skill, if data corpus is more than 200k instances, using the below codes will help to improve the loading efficiency giganticly\n",
    "## for json filesm transfer it into the arrow format using save_to_disk\n",
    "# dataset.save_to_disk(save_path)\n",
    "# dataset = load_from_disk(save_path)\n",
    "## In the map function\n",
    "# dataset = dataset.map(map_fn, num_proc=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859b377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['post', 'gpt_answer'],\n",
       "    num_rows: 97\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus = pd.read_csv('dataset/fine-tune-corpus-NLE.csv')\n",
    "df_corpus = df_corpus[3:].reset_index(drop=True)\n",
    "dataset = Dataset.from_pandas(df_corpus)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f7593c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb260e80db6d421eb5d7b34a0d161945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/fanhuan/cache/llama-2-13b-chat-hf and are newly initialized: ['model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# # Dataset\n",
    "# data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"/home/fanhuan/cache/llama-2-13b-chat-hf\"\n",
    "refined_model = \"/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA-IHS\"\n",
    "cache_dir = \"/data/fanhuan/cache/temp/13b-lora-IHS\"\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "#     device_map={\"\": 7}\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23757e",
   "metadata": {},
   "source": [
    "How to use the neptune features in Transformers:\n",
    "\n",
    "https://docs.neptune.ai/integrations/transformers/#__tabbed_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b709157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4170a8e9515437899fa3aecfbac1adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.823900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.6655711097717285, metrics={'train_runtime': 69.3955, 'train_samples_per_second': 6.989, 'train_steps_per_second': 1.801, 'total_flos': 2135737904302080.0, 'train_loss': 0.6655711097717285, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.integrations import NeptuneCallback\n",
    "# import neptune\n",
    "\n",
    "# run = neptune.init_run(\n",
    "#     project=\"fhuang181/LoRA\",\n",
    "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZmI2ZTA2OC00ZGIxLTQ2NDktYTU4YS1jOWUyNWIwYmU3YWUifQ==\",\n",
    "# )\n",
    "\n",
    "# neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=cache_dir,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    # very important setting for keep the disk space enough for further training\n",
    "    save_total_limit = 1,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # use the report_to parameter to avoid error in neptune stuff\n",
    "    # report_to=\"none\"\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Adding the format of SFTTrainer based on the columns of dataset loaded\n",
    "# https://huggingface.co/docs/trl/sft_trainer\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['post'])):\n",
    "        text = f\"### Question: Given the short text of: {example['post'][i]}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\\n ### Answer:\\n{example['gpt_answer'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"### Answer:\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=llama_tokenizer)\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_parameters,\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    "    ## if we need to use customized training corpus, it is better to use formatting_func and data_collator \n",
    "    # dataset_text_field=\"text\",\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "#     callbacks=[neptune_callback]\n",
    ")\n",
    "\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6407b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA-IHS/tokenizer_config.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA-IHS/special_tokens_map.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA-IHS/tokenizer.model',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA-IHS/added_tokens.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF-LoRA-IHS/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(refined_model)\n",
    "fine_tuning.tokenizer.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce31ab69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST]  The OpenAI API is a powerful tool that allows developers to access and interact with the AI models developed by OpenAI. Here's a step-by-step guide on how to use the OpenAI API:\n",
      "\n",
      "1. Sign up for an OpenAI account: To use the OpenAI API, you need to sign up for an OpenAI account. You can sign up for free on the OpenAI website.\n",
      "2. Create a new project: Once you have an OpenAI account, you can create a new project. To do this, click on the \"Create a new project\" button on the OpenAI dashboard.\n",
      "3. Select the AI model you want to use: OpenAI offers a variety of AI models, including language models, image models, and more. Select the model you want to use for your project.\n",
      "4. Set up the API endpoint\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=fine_tuning.model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1bca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST]  The OpenAI API is a powerful tool that allows developers to access and interact with the AI models developed by OpenAI. Here's a step-by-step guide on how to use the OpenAI API:\n",
      "\n",
      "1. Sign up for an OpenAI account: To use the OpenAI API, you need to sign up for an OpenAI account. You can sign up for free on the OpenAI website.\n",
      "2. Create a new project: Once you have an OpenAI account, you can create a new project. To do this, click on the \"Create a new project\" button on the OpenAI dashboard.\n",
      "3. Select the API you want to use: OpenAI offers several APIs, including the text generation API, the image generation API, and the dialogue generation API. Select the API you want to use for your project.\n",
      "4. Get an API\n"
     ]
    }
   ],
   "source": [
    "# Original model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b219263",
   "metadata": {},
   "source": [
    "### Due to the issue of: The model 'PeftModelForCausalLM' is not supported for text-generation.\n",
    "\n",
    "There is no specific difference for the pipeline code in two model settings, it is due to the error of PeftModelForCausalLM is not supported yet in Transformers pipelines.\n",
    "\n",
    "According to (https://huggingface.co/bertin-project/bertin-alpaca-lora-7b/discussions/1), it is better to simply use the generate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408dc538",
   "metadata": {},
   "source": [
    "## Controlled generation, via generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ccc4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OpenAI is a research organization that aims to promote the development of safe and beneficial artificial intelligence. As part of its mission, OpenAI has made its API available to the public, allowing developers to access its models and data for a variety of purposes.\n",
      "However, using the OpenAI API can be a bit challenging, as it requires a good understanding of the technology and the specific application you're trying to build. Here's a step-by-step guide to help you get started with the OpenAI API:\n",
      "\n",
      "1. Sign up for an account: To use the OpenAI API, you need to sign up for an account on the OpenAI website. Once you have an account, you can create a new project and get an API key, which you'll need to use to authenticate your API requests.\n",
      "2. Choose a model: OpenAI offers a variety of pre-trained models for different tasks, such as text classification, language translation, and question answering. You can browse the models available on the OpenAI website and choose the one that best fits your needs.\n",
      "3. Make API requests: Once you have selected a model, you can use the OpenAI API to make requests to the model. The API supports both plain text and JSON input/output formats. You can use the OpenAI API client library for Python or the OpenAI API command-line interface to make API requests.\n",
      "4. Authenticate your requests: To use the OpenAI API, you need to authenticate your requests with your API key. You can do this by including your API key in the headers of your API requests.\n",
      "5. Use the model: Once you have authenticated your requests, you can use the model to perform the task you need. For example, if you've selected a text classification model, you can use it to classify text inputs.\n",
      "6. Evaluate the results: After using the model, you should evaluate the results to ensure that the model is performing correctly. You can use metrics such as accuracy, precision, and recall to evaluate the performance of the model.\n",
      "7. Consider the limitations: While the OpenAI API is powerful, it has some limitations. For example, the models available on the OpenAI API are pre-trained, which means they may not be tailored to your specific needs. Additionally, the API has rate limits, which means you can only\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - before fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = base_model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f28340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The OpenAI API allows you to access OpenAI models and data through a restful interface. To use the OpenAI API, you will need to sign up for an account and obtain an access token. Once you have an access token, you can make requests to the API using HTTP requests.\n",
      "\n",
      "Here is an example of how to make a request to the OpenAI API:\n",
      "```\n",
      "GET /v1/models/latest/generate?prompt=BioSynthetic+Speech+Framework+for+Speech+and+Language+Therapy&temperature=0.8&max_tokens=50&model_name=ada-language-model&output_type=text&chat_id=123456 HTTP/1.1\n",
      "Host: api.openai.com\n",
      "Authorization: Bearer YOUR_ACCESS_TOKEN\n",
      "\n",
      "```\n",
      "This request generates text using the Ada Language Model, with the prompt \"BioSynthetic Speech Framework for Speech and Language Therapy\", the temperature parameter set to 0.8, and the maximum number of tokens set to 50. The output type is set to text and the chat ID is set to 123456.\n",
      "\n",
      "You can also use the OpenAI API to make other types of requests, such as retrieving information about models and data, or training your own models.\n",
      "\n",
      "Keep in mind that the OpenAI API has usage limits and restrictions on commercial use, so be sure to review the terms of service before using the API. Additionally, the OpenAI API is a research-oriented API and not intended for production use, so use it with caution and at your own risk.</s>\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - after fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = fine_tuning.model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03899d7a",
   "metadata": {},
   "source": [
    "# Fine-grained performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd0afc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>human_answer</th>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <th>llama2-13b-chat-FT-LoRA</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <th>llama2-7b-FT-LoRA</th>\n",
       "      <th>llama2-7b-FT</th>\n",
       "      <th>llama2-13b-chat-FT</th>\n",
       "      <th>llama2-13b-chat-FT-LoRA-IHS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and i will point it out here when u call whit...</td>\n",
       "      <td>Blacks blame whites</td>\n",
       "      <td>\\n\\nSo for example, if you would answer Yes, p...</td>\n",
       "      <td>\\n\\nYes: Explain that this text is an example ...</td>\n",
       "      <td>\\n\\nAnswer: I do not think implicit hate speec...</td>\n",
       "      <td>\\nThank you, I greatly appreciate any and all ...</td>\n",
       "      <td>[/INST] You have any further or other question...</td>\n",
       "      <td>,. -. a-,,\\n as of and to is the, and-. of. th...</td>\n",
       "      <td>\\nThis post implies that calling someone a whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiwhites expose their hatred of white kids ...</td>\n",
       "      <td>Anti-whites hate white people</td>\n",
       "      <td>\\n\\nOf course, I won't be offended by your ans...</td>\n",
       "      <td>\\n\\nNote that if you do not see any implicit h...</td>\n",
       "      <td>\\n### No \\nThis text was not containing any ha...</td>\n",
       "      <td>\\n\\n\\n\\nHere is the text: \\n\\n\\nHey buddy what...</td>\n",
       "      <td>Can you explain? - - - - - - - - - - - - - - ...</td>\n",
       "      <td>and for the,en of ( as, is the\":, and en and o...</td>\n",
       "      <td>The context of the post is not given, but it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interior sec y salazar pushing for hispanic n...</td>\n",
       "      <td>Minorities are taking over society</td>\n",
       "      <td>So that it's clear to others what was the reas...</td>\n",
       "      <td>\\n\\nHere is the short text: and i will point i...</td>\n",
       "      <td>\\n- It's clear this text is containing implici...</td>\n",
       "      <td>Thank you.\\n\\nI hope that isn't too long üëåüèºüíñ\\n...</td>\n",
       "      <td>It Is also deliberately deliberately misreali...</td>\n",
       "      <td>, that a which as,, a,,\\n./3. that the and of,...</td>\n",
       "      <td>This post is implicitly hateful because it use...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post   \n",
       "0   and i will point it out here when u call whit...  \\\n",
       "1   antiwhites expose their hatred of white kids ...   \n",
       "2   interior sec y salazar pushing for hispanic n...   \n",
       "\n",
       "                         human_answer   \n",
       "0                 Blacks blame whites  \\\n",
       "1       Anti-whites hate white people   \n",
       "2  Minorities are taking over society   \n",
       "\n",
       "                                     llama2-13b-chat   \n",
       "0  \\n\\nSo for example, if you would answer Yes, p...  \\\n",
       "1  \\n\\nOf course, I won't be offended by your ans...   \n",
       "2  So that it's clear to others what was the reas...   \n",
       "\n",
       "                             llama2-13b-chat-FT-LoRA   \n",
       "0  \\n\\nYes: Explain that this text is an example ...  \\\n",
       "1  \\n\\nNote that if you do not see any implicit h...   \n",
       "2  \\n\\nHere is the short text: and i will point i...   \n",
       "\n",
       "                                           llama2-7b   \n",
       "0  \\n\\nAnswer: I do not think implicit hate speec...  \\\n",
       "1  \\n### No \\nThis text was not containing any ha...   \n",
       "2  \\n- It's clear this text is containing implici...   \n",
       "\n",
       "                                   llama2-7b-FT-LoRA   \n",
       "0  \\nThank you, I greatly appreciate any and all ...  \\\n",
       "1  \\n\\n\\n\\nHere is the text: \\n\\n\\nHey buddy what...   \n",
       "2  Thank you.\\n\\nI hope that isn't too long üëåüèºüíñ\\n...   \n",
       "\n",
       "                                        llama2-7b-FT   \n",
       "0  [/INST] You have any further or other question...  \\\n",
       "1   Can you explain? - - - - - - - - - - - - - - ...   \n",
       "2   It Is also deliberately deliberately misreali...   \n",
       "\n",
       "                                  llama2-13b-chat-FT   \n",
       "0  ,. -. a-,,\\n as of and to is the, and-. of. th...  \\\n",
       "1  and for the,en of ( as, is the\":, and en and o...   \n",
       "2  , that a which as,, a,,\\n./3. that the and of,...   \n",
       "\n",
       "                         llama2-13b-chat-FT-LoRA-IHS  \n",
       "0  \\nThis post implies that calling someone a whi...  \n",
       "1  The context of the post is not given, but it's...  \n",
       "2  This post is implicitly hateful because it use...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/implicit-hate-speech.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79dad217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:17<00:00,  5.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "#     # original model \n",
    "#     model = base_model\n",
    "#     tokenizer = llama_tokenizer\n",
    "    \n",
    "#     tweet = df.loc[0,'post']\n",
    "\n",
    "#     text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "#     inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "#     outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "#                                                pad_token_id=tokenizer.eos_token_id,\n",
    "#                                                eos_token_id=tokenizer.eos_token_id,\n",
    "#                                                return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "#     input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "#     generated_tokens = outputs.sequences[:, input_length:]\n",
    "#     temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "#     df.loc[i, 'llama2-13b-chat'] = temp_output\n",
    "    \n",
    "    # fine-tuned model, using LoRA\n",
    "    model = fine_tuning.model\n",
    "    tokenizer = llama_tokenizer\n",
    "    \n",
    "    tweet = df.loc[0,'post']\n",
    "\n",
    "    text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                               pad_token_id=tokenizer.eos_token_id,\n",
    "                                               eos_token_id=tokenizer.eos_token_id,\n",
    "                                               return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "    df.loc[i, 'llama2-13b-chat-FT-LoRA-IHS'] = temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1e5a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/implicit-hate-speech.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce5d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
