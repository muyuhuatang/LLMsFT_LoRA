{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43adad9b",
   "metadata": {},
   "source": [
    "# Fine-tune Llama2 with LoRA for QA\n",
    "\n",
    "\n",
    "reference code: https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf7d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('error', category=DeprecationWarning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3cb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90400e99",
   "metadata": {},
   "source": [
    "Error [NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported] resolving:\n",
    "\n",
    "1. pip install -U datasets\n",
    "2. pip install fsspec==2023.9.2\n",
    "3. restart the kernel of this jupyter notebook\n",
    "\n",
    "https://stackoverflow.com/questions/77433096/notimplementederror-loading-a-dataset-cached-in-a-localfilesystem-is-not-suppor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f7593c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773d896b36754668a434d9955ac0d537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/fanhuan/cache/llama-2-7b-hf and are newly initialized: ['model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Model and tokenizer names\n",
    "# base_model_name = \"/home/fanhuan/cache/llama-2-13b-chat-hf\"\n",
    "# refined_model = \"/home/fanhuan/cache/llama-2-13b-chat-hf-TF\"\n",
    "base_model_name = \"/home/fanhuan/cache/llama-2-7b-hf\"\n",
    "refined_model = \"/home/fanhuan/cache/llama-2-7b-chat-hf-TF\"\n",
    "cache_dir = \"/data/fanhuan/cache/temp/7b-lora\"\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "#     device_map={\"\": 7}\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23757e",
   "metadata": {},
   "source": [
    "How to use the neptune features in Transformers:\n",
    "\n",
    "https://docs.neptune.ai/integrations/transformers/#__tabbed_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b709157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.470600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.2999353637695312, metrics={'train_runtime': 297.7127, 'train_samples_per_second': 3.359, 'train_steps_per_second': 0.84, 'total_flos': 8698296253906944.0, 'train_loss': 1.2999353637695312, 'epoch': 1.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.integrations import NeptuneCallback\n",
    "# import neptune\n",
    "\n",
    "# run = neptune.init_run(\n",
    "#     project=\"fhuang181/LoRA\",\n",
    "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZmI2ZTA2OC00ZGIxLTQ2NDktYTU4YS1jOWUyNWIwYmU3YWUifQ==\",\n",
    "# )\n",
    "\n",
    "# neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=cache_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    # very important setting for keep the disk space enough for further training\n",
    "    save_total_limit = 1,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # use the report_to parameter to avoid error in neptune stuff\n",
    "    # report_to=\"none\"\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    "    # callbacks=[neptune_callback]\n",
    ")\n",
    "\n",
    "fine_tuning.train()\n",
    "\n",
    "# Save Model\n",
    "# fine_tuning.model.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d6407b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/fanhuan/cache/llama-2-7b-chat-hf-TF/tokenizer_config.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-chat-hf-TF/special_tokens_map.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-chat-hf-TF/tokenizer.model',\n",
       " '/home/fanhuan/cache/llama-2-7b-chat-hf-TF/added_tokens.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-chat-hf-TF/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(refined_model)\n",
    "fine_tuning.tokenizer.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce31ab69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST] The OpenAI API is a RESTful API that allows you to interact with the OpenAI models. OpenAI provides a Python SDK that makes it easy to interact with the API. You can also use the OpenAI API with other programming languages, such as JavaScript, Java, and Ruby. \n",
      "\n",
      "To use the OpenAI API, you need to create an OpenAI API key. You can do this by visiting the OpenAI API key page and following the instructions. Once you have an API key, you can use it to make requests to the OpenAI API. \n",
      "\n",
      "To make a request to the OpenAI API, you need to construct a URL that includes your API key and the parameters for the request. For example, to get a list of all the models available in the API, you would construct a URL like this:\n",
      "\n",
      "```\n",
      "https://api.openai.com\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=fine_tuning.model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb1bca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST] The OpenAI API is a RESTful API that allows you to interact with the OpenAI models. OpenAI provides a Python SDK that makes it easy to interact with the API. You can also use the OpenAI API with other programming languages, such as JavaScript, Java, and Ruby. \n",
      "\n",
      "To use the OpenAI API, you need to create an OpenAI API key. You can do this by visiting the OpenAI API key page and following the instructions. Once you have an API key, you can use it to make requests to the OpenAI API. \n",
      "\n",
      "To make a request to the OpenAI API, you need to construct a URL that includes your API key and the parameters for the request. For example, to get a list of all the models available, you would construct a URL like this:\n",
      "\n",
      "```\n",
      "https://api.openai.com/v1\n"
     ]
    }
   ],
   "source": [
    "# Original model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b219263",
   "metadata": {},
   "source": [
    "### Due to the issue of: The model 'PeftModelForCausalLM' is not supported for text-generation.\n",
    "\n",
    "There is no specific difference for the pipeline code in two model settings, it is due to the error of PeftModelForCausalLM is not supported yet in Transformers pipelines.\n",
    "\n",
    "According to (https://huggingface.co/bertin-project/bertin-alpaca-lora-7b/discussions/1), it is better to simply use the generate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408dc538",
   "metadata": {},
   "source": [
    "## Controlled generation, via generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ccc4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " hopefully that makes sense.\n",
      "I am new to python and trying to learn it so any help is much appreciated.\n",
      "I believe I have to use curl or something, maybe I have to install a python package for this?\n",
      "I don't know what to install, can someone help me.</s>\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - before fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = base_model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f28340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖⚙️\n",
      "  You connect with an OpenAI API Key, then the OpenAI Python SDK (or OpenAI CLI if you are on Linux).  OpenAI Python SDK docs: https://openai.com/docs/api-reference/  OpenAI.CLI docs: https://openai.com/docs/cli/usage/openai-cli-examples/  OpenAI Python SDK docs: https://openai.com/docs/api-reference/  How do I get started with OpenAI API Keys? 🗂️⚙️  You can register for an OpenAI API Key from the OpenAI Developer Portal: https://beta.developer.openai.com/openapi-keys/create. If you do not see an activation URL, this means you need to wait for API access to start and your key is pending. Once you receive the activation URL, you should copy it for use.  How do I connect my OpenAI API Key for CLI use? 🏗⚙️  Create a file in ~/.openai-config (any file name, but it should contain your API Key).  This location is recommended because it allows for easy update of your key if changes are needed.    \n",
      "```CLOUD.OAUTH2 {  TOKEN=YOURTOKENHERE  }  ```  The recommended location is \"~/.openai-config, but you may want to put it somewhere more appropriate.  You can change the ~/.openai-config file to whatever path is more appropriate, but in the case the environment var openai_config_path is not set, ~/.openai-config is assumed.  How do I use Python? 🤖⚙️  You can install the OpenAI Python SDK using an OpenAI CLI command. OpenAI CLI docs: https://openai.com/docs/cli/usage/openai-cli-examples/  OpenAI Python SDK docs: https://openai.com/docs/api-reference/  The OpenAI Python SDK allows you to control all OpenAI API-related requests, and it includes a large list of features that are easier to work with than those on the OpenAI API.  How do I use the gpt-3-openai-api? ���\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - after fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = fine_tuning.model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03899d7a",
   "metadata": {},
   "source": [
    "# Fine-grained performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd0afc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>human_answer</th>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <th>llama2-13b-chat-FT-LoRA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and i will point it out here when u call whit...</td>\n",
       "      <td>Blacks blame whites</td>\n",
       "      <td>\\n\\nSo for example, if you would answer Yes, p...</td>\n",
       "      <td>\\n\\nYes: Explain that this text is an example ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiwhites expose their hatred of white kids ...</td>\n",
       "      <td>Anti-whites hate white people</td>\n",
       "      <td>\\n\\nOf course, I won't be offended by your ans...</td>\n",
       "      <td>\\n\\nNote that if you do not see any implicit h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interior sec y salazar pushing for hispanic n...</td>\n",
       "      <td>Minorities are taking over society</td>\n",
       "      <td>So that it's clear to others what was the reas...</td>\n",
       "      <td>\\n\\nHere is the short text: and i will point i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post   \n",
       "0   and i will point it out here when u call whit...  \\\n",
       "1   antiwhites expose their hatred of white kids ...   \n",
       "2   interior sec y salazar pushing for hispanic n...   \n",
       "\n",
       "                         human_answer   \n",
       "0                 Blacks blame whites  \\\n",
       "1       Anti-whites hate white people   \n",
       "2  Minorities are taking over society   \n",
       "\n",
       "                                     llama2-13b-chat   \n",
       "0  \\n\\nSo for example, if you would answer Yes, p...  \\\n",
       "1  \\n\\nOf course, I won't be offended by your ans...   \n",
       "2  So that it's clear to others what was the reas...   \n",
       "\n",
       "                             llama2-13b-chat-FT-LoRA  \n",
       "0  \\n\\nYes: Explain that this text is an example ...  \n",
       "1  \\n\\nNote that if you do not see any implicit h...  \n",
       "2  \\n\\nHere is the short text: and i will point i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/implicit-hate-speech.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79dad217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 3/3 [04:52<00:00, 97.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "    # original model \n",
    "    model = base_model\n",
    "    tokenizer = llama_tokenizer\n",
    "    \n",
    "    tweet = df.loc[0,'post']\n",
    "\n",
    "    text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                               pad_token_id=tokenizer.eos_token_id,\n",
    "                                               eos_token_id=tokenizer.eos_token_id,\n",
    "                                               return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "    df.loc[i, 'llama2-7b'] = temp_output\n",
    "    \n",
    "    # fine-tuned model, using LoRA\n",
    "    model = fine_tuning.model\n",
    "    tokenizer = llama_tokenizer\n",
    "    \n",
    "    tweet = df.loc[0,'post']\n",
    "\n",
    "    text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                               pad_token_id=tokenizer.eos_token_id,\n",
    "                                               eos_token_id=tokenizer.eos_token_id,\n",
    "                                               return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "    df.loc[i, 'llama2-7b-FT-LoRA'] = temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1e5a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/implicit-hate-speech.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fda9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
