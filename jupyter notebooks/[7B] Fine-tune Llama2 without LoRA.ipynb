{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43adad9b",
   "metadata": {},
   "source": [
    "# Fine-tune Llama2 without LoRA for QA\n",
    "\n",
    "\n",
    "reference code: https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf7d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('error', category=DeprecationWarning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3cb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90400e99",
   "metadata": {},
   "source": [
    "Error [NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported] resolving:\n",
    "\n",
    "1. pip install -U datasets\n",
    "2. pip install fsspec==2023.9.2\n",
    "3. restart the kernel of this jupyter notebook\n",
    "\n",
    "https://stackoverflow.com/questions/77433096/notimplementederror-loading-a-dataset-cached-in-a-localfilesystem-is-not-suppor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6f7593c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcc26309b36437db822dd0e26825c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/fanhuan/cache/llama-2-7b-hf and are newly initialized: ['model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Model and tokenizer names\n",
    "# base_model_name = \"/home/fanhuan/cache/llama-2-13b-chat-hf\"\n",
    "# refined_model = \"/home/fanhuan/cache/llama-2-13b-chat-hf-TF\"\n",
    "base_model_name = \"/home/fanhuan/cache/llama-2-7b-hf\"\n",
    "refined_model = \"/home/fanhuan/cache/llama-2-7b-chat-hf-TF\"\n",
    "cache_dir = \"/data/fanhuan/cache/temp/7b\"\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "# If do not use the LoRA, it is better to turn the 'load_in_4bit' into False\n",
    "#, since the source code of this part is not perfect right now, Apr-30-2024\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "#     device_map={\"\": 0}\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23757e",
   "metadata": {},
   "source": [
    "How to use the neptune features in Transformers:\n",
    "\n",
    "https://docs.neptune.ai/integrations/transformers/#__tabbed_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b709157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 16:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.854900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>4.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.903300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.046000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=3.9481307678222657, metrics={'train_runtime': 977.0586, 'train_samples_per_second': 1.023, 'train_steps_per_second': 0.256, 'total_flos': 1.7036321920745472e+16, 'train_loss': 3.9481307678222657, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.integrations import NeptuneCallback\n",
    "# import neptune\n",
    "\n",
    "# run = neptune.init_run(\n",
    "#     project=\"fhuang181/LoRA\", \n",
    "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZmI2ZTA2OC00ZGIxLTQ2NDktYTU4YS1jOWUyNWIwYmU3YWUifQ==\", # your credentials\n",
    "# )\n",
    "\n",
    "# neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "# # LoRA Config\n",
    "# peft_parameters = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=8,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=cache_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    # very important setting for keep the disk space enough for further training\n",
    "    save_total_limit = 1,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False, # NotImplementedError: You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # use the report_to parameter to avoid error in neptune stuff\n",
    "    # report_to=\"none\"\n",
    "    report_to=\"tensorboard\"\n",
    "    # report_to=\"neptune\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    # model = check_point_model,\n",
    "    train_dataset=training_data,\n",
    "    # peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    "    # callbacks=[neptune_callback]\n",
    ")\n",
    "\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d6407b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/fanhuan/cache/llama-2-7b-chat-hf-TF/tokenizer_config.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-chat-hf-TF/special_tokens_map.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-chat-hf-TF/tokenizer.model',\n",
       " '/home/fanhuan/cache/llama-2-7b-chat-hf-TF/added_tokens.json',\n",
       " '/home/fanhuan/cache/llama-2-7b-chat-hf-TF/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(refined_model)\n",
    "fine_tuning.tokenizer.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce31ab69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST] I can help you use the OpenAI, or I can help you use the OpenAI, using various methods. \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including:\n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including: \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including: \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including: \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including: \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=fine_tuning.model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb1bca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST] I can help you use the OpenAI, or I can help you use the OpenAI, using various methods. \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including:\n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including: \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including: \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including: \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI, using various methods, including: \n",
      "\n",
      "I can help you use the OpenAI, or I can help you use the OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Original model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b219263",
   "metadata": {},
   "source": [
    "### Due to the issue of: The model 'PeftModelForCausalLM' is not supported for text-generation.\n",
    "\n",
    "There is no specific difference for the pipeline code in two model settings, it is due to the error of PeftModelForCausalLM is not supported yet in Transformers pipelines.\n",
    "\n",
    "According to (https://huggingface.co/bertin-project/bertin-alpaca-lora-7b/discussions/1), it is better to simply use the generate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408dc538",
   "metadata": {},
   "source": [
    "## Controlled generation, via generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ccc4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/INST] I can ask you to provide what you need, to use, or what you would like to use, which will be helpful for you, if you have more help or use needed. \n",
      "I can also ask you about various tasks or questions you would not like, \n",
      "It's worth not what you want me, \n",
      "\n",
      "I can also help or use what you want, if you have any other area or if you need,, that'd be helpful, I might have, the use you'd be helpful has you like, the use you might want, I'd be helpful may be helpful. \n",
      "En què, \n",
      "En el área of what you want, I would like, I'd be helpful, you might have, I have neednir help or use, let me need, I might have, you'd be helpful, I have asked, I have asked, \n",
      "Sentence - Open.  - - Explainable, \n",
      "\n",
      "The phrase \"Claroise what you did.  - Give me - I'd be helpful.\"  =  \n",
      "\n",
      "We would like to help or use, I have asked, ask me, if I have any other area or inquiry you you need or, if - - or - - - I have asked, I have asked, you have asked, what you have asked, or what I have asked, I have asked, it's been helpful.  - \n",
      "I have ask, you have asked, or if you have asked, it'd be helpful,,  I have asked, you have asked (and/or - no], I - - or - -). \n",
      "Thile, I have ask, I have asked, what my has asked, or what this\" - I have asked, i I have asked, I have asked, has been helpful or, if - it''s been helpful,, if - if - - - - - this have asked, I have asked, or there'ate, I have asked, \n",
      "\n",
      "However, I also need assistance or assistance, I'd be helpful, if you'd like,, you have asked, if you have asked, it in a area or area, I'd have asked, or if - it's been helpful,, no matter how it's been asked, or no matter emocció\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - before fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = base_model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5f28340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any people with the United States? I doesn' have many people I'd like to connect with. Please include people you know with your [S]. I'd also like to connect at [The moment, I'd like to connect.\" \n",
      "Do you have more of what you'd like to connect or, just consider the moment or consider? \n",
      "Do you have further inquiry or query, please, I'd or consider, I'd or connect, I would like, I'd \n",
      "Do you have any further inquiry or or inquiring, I'd, or if no you'd, you'd, you'd, or it doesn', I'd, I, or \n",
      "Do you have any other inquiry or inquiry, please, I would or connect? I'd or connect, please,, I, or what other of my [day] I'd or connect, please, I have, I would, and the [nicht quite] person I need it?\" \n",
      "Note that I cannot connect as well as other. \n",
      "Note that I may have an average total length of minutes or minutes, as well as an average total length of hours or moment, as to what a person is or is being do, or to what a [nicht average]. \n",
      "\"Sincroniza with your \"nicht average\", we will connect as a result of, for what no person or government would or connect.\"  \n",
      "Suffixes for the \"nicht average\" \n",
      "You will connect with, and with what no you need,\n",
      " \n",
      "Suffixes for the \"nicht average\", as well as people, you need, \n",
      " \n",
      "We will connect with, and with what no you need, will help? \n",
      " \n",
      "Fantay, \n",
      "\"Cableviatura or cableviatura or device, if you need or connection, you want,\" \n",
      "En resumen, we will connect.  \n",
      "\n",
      "Suffixes for to humanity and society in \n",
      "Suffixes, you want, \n",
      "  \n",
      "The most empathy that you amie,  (UW) will provide you. \n",
      "No, I have no people or inquiry, as to what no person or person you have, or \n",
      "No, I have no\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - after fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = fine_tuning.model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03899d7a",
   "metadata": {},
   "source": [
    "# Fine-grained performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd0afc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>human_answer</th>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <th>llama2-13b-chat-FT-LoRA</th>\n",
       "      <th>llama2-7b-chat</th>\n",
       "      <th>llama2-7b-FT-LoRA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and i will point it out here when u call whit...</td>\n",
       "      <td>Blacks blame whites</td>\n",
       "      <td>\\n\\nSo for example, if you would answer Yes, p...</td>\n",
       "      <td>\\n\\nYes: Explain that this text is an example ...</td>\n",
       "      <td>\\n\\nAnswer: I do not think implicit hate speec...</td>\n",
       "      <td>\\nThank you, I greatly appreciate any and all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiwhites expose their hatred of white kids ...</td>\n",
       "      <td>Anti-whites hate white people</td>\n",
       "      <td>\\n\\nOf course, I won't be offended by your ans...</td>\n",
       "      <td>\\n\\nNote that if you do not see any implicit h...</td>\n",
       "      <td>\\n### No \\nThis text was not containing any ha...</td>\n",
       "      <td>\\n\\n\\n\\nHere is the text: \\n\\n\\nHey buddy what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interior sec y salazar pushing for hispanic n...</td>\n",
       "      <td>Minorities are taking over society</td>\n",
       "      <td>So that it's clear to others what was the reas...</td>\n",
       "      <td>\\n\\nHere is the short text: and i will point i...</td>\n",
       "      <td>\\n- It's clear this text is containing implici...</td>\n",
       "      <td>Thank you.\\n\\nI hope that isn't too long 👌🏼💖\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post   \n",
       "0   and i will point it out here when u call whit...  \\\n",
       "1   antiwhites expose their hatred of white kids ...   \n",
       "2   interior sec y salazar pushing for hispanic n...   \n",
       "\n",
       "                         human_answer   \n",
       "0                 Blacks blame whites  \\\n",
       "1       Anti-whites hate white people   \n",
       "2  Minorities are taking over society   \n",
       "\n",
       "                                     llama2-13b-chat   \n",
       "0  \\n\\nSo for example, if you would answer Yes, p...  \\\n",
       "1  \\n\\nOf course, I won't be offended by your ans...   \n",
       "2  So that it's clear to others what was the reas...   \n",
       "\n",
       "                             llama2-13b-chat-FT-LoRA   \n",
       "0  \\n\\nYes: Explain that this text is an example ...  \\\n",
       "1  \\n\\nNote that if you do not see any implicit h...   \n",
       "2  \\n\\nHere is the short text: and i will point i...   \n",
       "\n",
       "                                      llama2-7b-chat   \n",
       "0  \\n\\nAnswer: I do not think implicit hate speec...  \\\n",
       "1  \\n### No \\nThis text was not containing any ha...   \n",
       "2  \\n- It's clear this text is containing implici...   \n",
       "\n",
       "                                   llama2-7b-FT-LoRA  \n",
       "0  \\nThank you, I greatly appreciate any and all ...  \n",
       "1  \\n\\n\\n\\nHere is the text: \\n\\n\\nHey buddy what...  \n",
       "2  Thank you.\\n\\nI hope that isn't too long 👌🏼💖\\n...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/implicit-hate-speech.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79dad217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3/3 [01:48<00:00, 36.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "#     # original model \n",
    "#     model = base_model\n",
    "#     tokenizer = llama_tokenizer\n",
    "    \n",
    "#     tweet = df.loc[0,'post']\n",
    "\n",
    "#     text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "#     inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "#     outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "#                                                pad_token_id=tokenizer.eos_token_id,\n",
    "#                                                eos_token_id=tokenizer.eos_token_id,\n",
    "#                                                return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "#     input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "#     generated_tokens = outputs.sequences[:, input_length:]\n",
    "#     temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "#     df.loc[i, 'llama2-13b-chat'] = temp_output\n",
    "    \n",
    "    # fine-tuned model, using LoRA\n",
    "    model = fine_tuning.model\n",
    "    tokenizer = llama_tokenizer\n",
    "    \n",
    "    tweet = df.loc[0,'post']\n",
    "\n",
    "    text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                               pad_token_id=tokenizer.eos_token_id,\n",
    "                                               eos_token_id=tokenizer.eos_token_id,\n",
    "                                               return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "    df.loc[i, 'llama2-7b-FT'] = temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1e5a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/implicit-hate-speech.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082179ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
