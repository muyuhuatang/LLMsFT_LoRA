{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43adad9b",
   "metadata": {},
   "source": [
    "##### Fine-tune Llama2 without LoRA for QA\n",
    "\n",
    "\n",
    "reference code: https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf7d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('error', category=DeprecationWarning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3cb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "import os, torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90400e99",
   "metadata": {},
   "source": [
    "Error [NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported] resolving:\n",
    "\n",
    "1. pip install -U datasets\n",
    "2. pip install fsspec==2023.9.2\n",
    "3. restart the kernel of this jupyter notebook\n",
    "\n",
    "https://stackoverflow.com/questions/77433096/notimplementederror-loading-a-dataset-cached-in-a-localfilesystem-is-not-suppor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f7593c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fe113e793940d6a4cdecd73a095611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/fanhuan/cache/llama-2-13b-chat-hf and are newly initialized: ['model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"/home/fanhuan/cache/llama-2-13b-chat-hf\"\n",
    "refined_model = \"/home/fanhuan/cache/llama-2-13b-chat-hf-TF\"\n",
    "# base_model_name = \"/home/fanhuan/cache/llama-2-7b-hf\"\n",
    "# refined_model = \"/home/fanhuan/cache/llama-2-7b-chat-hf-TF\"\n",
    "cache_dir = \"/data/fanhuan/cache/temp/13b\"\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "\n",
    "# Quantization Config\n",
    "# If do not use the LoRA, it is better to turn the 'load_in_4bit' into False\n",
    "#, since the source code of this part is not perfect right now, Apr-30-2024\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "#     device_map={\"\": 0}\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23757e",
   "metadata": {},
   "source": [
    "How to use the neptune features in Transformers:\n",
    "\n",
    "https://docs.neptune.ai/integrations/transformers/#__tabbed_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b709157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 58:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.842500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>8.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>8.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>7.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>7.865200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>7.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>7.286700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=7.068787139892578, metrics={'train_runtime': 3573.8231, 'train_samples_per_second': 0.28, 'train_steps_per_second': 0.07, 'total_flos': 3.313755669049344e+16, 'train_loss': 7.068787139892578, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.integrations import NeptuneCallback\n",
    "# import neptune\n",
    "\n",
    "# run = neptune.init_run(\n",
    "#     project=\"fhuang181/LoRA\", \n",
    "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZmI2ZTA2OC00ZGIxLTQ2NDktYTU4YS1jOWUyNWIwYmU3YWUifQ==\", # your credentials\n",
    "# )\n",
    "\n",
    "# neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "# # LoRA Config\n",
    "# peft_parameters = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=8,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=cache_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    # very important setting for keep the disk space enough for further training\n",
    "    save_total_limit = 1,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # use the report_to parameter to avoid error in neptune stuff\n",
    "    # report_to=\"none\"\n",
    "    report_to=\"tensorboard\"\n",
    "    # report_to=\"neptune\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    # peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    "    # callbacks=[neptune_callback]\n",
    ")\n",
    "\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6407b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/fanhuan/cache/llama-2-13b-chat-hf-TF/tokenizer_config.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF/special_tokens_map.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF/tokenizer.model',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF/added_tokens.json',\n",
       " '/home/fanhuan/cache/llama-2-13b-chat-hf-TF/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(refined_model)\n",
    "fine_tuning.tokenizer.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce31ab69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST] is is the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=fine_tuning.model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1bca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I use the OpenAI API? [/INST] is is the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "# Original model\n",
    "prompt = \"How do I use the OpenAI API?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b219263",
   "metadata": {},
   "source": [
    "### Due to the issue of: The model 'PeftModelForCausalLM' is not supported for text-generation.\n",
    "\n",
    "There is no specific difference for the pipeline code in two model settings, it is due to the error of PeftModelForCausalLM is not supported yet in Transformers pipelines.\n",
    "\n",
    "According to (https://huggingface.co/bertin-project/bertin-alpaca-lora-7b/discussions/1), it is better to simply use the generate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408dc538",
   "metadata": {},
   "source": [
    "## Controlled generation, via generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ccc4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. to of to\n",
      " in and the, the/ and3 the  an.,. the,.\n",
      "\n",
      " be the and, of.,. is\n",
      " and the a a\n",
      " and. \" un the in from ( to'.\". the,en\n",
      "..ic for, de The  \" I-:. can\"  or as and of . no., \" of is the of'./ a  as  \n",
      " and. ,, \", as [ the. and  as. to\n",
      "\n",
      "' of is  the \" within s. \"3. for  and, the  \" to .s is of, as the, the. to/ andes,ing, of\n",
      " and. is the. notic' or which not can./.\n",
      " and-'s and\n",
      " has  of withs I the.INST] The is is of is and is The as to \" I which you the the is the Earth\n",
      " of are the is the to a, [ of and is not the the the \" of the about the the' is is, The the not a the The, of the is ' the a \" to of the the the the  and the is is the the The the is an in of and and the and the is the the and with for the the is the the the the the information the if some a the the the of  the the the as. the is The that about which\n",
      ", the the. the a the is the \" the the all the the a is also. that of the as and the the is of is and\n",
      " the the the the a the the' The the The to the which the  the, and of the this and and the or the is of the the also the the the  and the or \" of which and not the the the:, and, of the from and it and the the' and, the to it the the, as and \n",
      " of as in the is the from the the, that of,\n",
      " and the are the,:. the, of the the and the and The the The of the as. and he and about\n",
      " the the and that the the the' the is and of1 as to, the the the the\n",
      " The the and and of of of and  a which and as and the, or on the the to which the to and Earth the the\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - before fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = base_model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f28340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " of)1 and that the as \",,, a of not, from , for not no-. the, and a \",  \",, and a\n",
      ". is is \"? be- in- foring. it no,ing an I,\n",
      " and.  the  is is the' the, de the in of a, :- has the which/.en to in,,.',, the is3,  the,/INST] What several is is is \" and that the has and can and the all to the of and like  is the thes or an the [ the which the is as the is to the is is  does iss the in the to of  or not are. I is the the of of is  a the this, between with or to a  froms the the as The is- to Earth the about of the, this the the the would with is has is the as. is the The ising' the and which not its the the is with The about is it is'\n",
      ", the to is and of the in of to which the the not to the a known the the not are the the is  the of is the in the, the the the is is the the about  that the is the,  and the and the, the he which a the the of this the or  or the and and is of the a in the for [ of is the the to\n",
      " the of  is ands which all and the is that the  the which The [ and \" is a the the the \" the the. of is are and the, is. The I' from  the the the is the is the, the a and the the' the has\n",
      " the by the the the and of from the with have the \" the and its and  the the its and be the the the the the the of the the the the and the or an of [, and the. of the of for which, to the and. information' as theing and \"  \"  the and the the the the. the  The, and the the the the I the the  to the. the\n",
      " is their  the  or the to information of the its the has or the  The of and of and in [ of the information\n",
      " a is the the the the of the of it the to,  the the the The with the a is\n"
     ]
    }
   ],
   "source": [
    "# Generate Text - after fine-tune\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "model = fine_tuning.model\n",
    "tokenizer = llama_tokenizer\n",
    "\n",
    "text = \"How do I use the OpenAI API?\"\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                           pad_token_id=tokenizer.eos_token_id,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "print(temp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03899d7a",
   "metadata": {},
   "source": [
    "# Fine-grained performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd0afc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>human_answer</th>\n",
       "      <th>llama2-13b-chat</th>\n",
       "      <th>llama2-13b-chat-FT-LoRA</th>\n",
       "      <th>llama2-7b</th>\n",
       "      <th>llama2-7b-FT-LoRA</th>\n",
       "      <th>llama2-7b-FT</th>\n",
       "      <th>llama2-13b-chat-FT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and i will point it out here when u call whit...</td>\n",
       "      <td>Blacks blame whites</td>\n",
       "      <td>\\n\\nSo for example, if you would answer Yes, p...</td>\n",
       "      <td>\\n\\nYes: Explain that this text is an example ...</td>\n",
       "      <td>\\n\\nAnswer: I do not think implicit hate speec...</td>\n",
       "      <td>\\nThank you, I greatly appreciate any and all ...</td>\n",
       "      <td>[/INST] You have any further or other question...</td>\n",
       "      <td>,. -. a-,,\\n as of and to is the, and-. of. th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiwhites expose their hatred of white kids ...</td>\n",
       "      <td>Anti-whites hate white people</td>\n",
       "      <td>\\n\\nOf course, I won't be offended by your ans...</td>\n",
       "      <td>\\n\\nNote that if you do not see any implicit h...</td>\n",
       "      <td>\\n### No \\nThis text was not containing any ha...</td>\n",
       "      <td>\\n\\n\\n\\nHere is the text: \\n\\n\\nHey buddy what...</td>\n",
       "      <td>Can you explain? - - - - - - - - - - - - - - ...</td>\n",
       "      <td>and for the,en of ( as, is the\":, and en and o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interior sec y salazar pushing for hispanic n...</td>\n",
       "      <td>Minorities are taking over society</td>\n",
       "      <td>So that it's clear to others what was the reas...</td>\n",
       "      <td>\\n\\nHere is the short text: and i will point i...</td>\n",
       "      <td>\\n- It's clear this text is containing implici...</td>\n",
       "      <td>Thank you.\\n\\nI hope that isn't too long üëåüèºüíñ\\n...</td>\n",
       "      <td>It Is also deliberately deliberately misreali...</td>\n",
       "      <td>, that a which as,, a,,\\n./3. that the and of,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post   \n",
       "0   and i will point it out here when u call whit...  \\\n",
       "1   antiwhites expose their hatred of white kids ...   \n",
       "2   interior sec y salazar pushing for hispanic n...   \n",
       "\n",
       "                         human_answer   \n",
       "0                 Blacks blame whites  \\\n",
       "1       Anti-whites hate white people   \n",
       "2  Minorities are taking over society   \n",
       "\n",
       "                                     llama2-13b-chat   \n",
       "0  \\n\\nSo for example, if you would answer Yes, p...  \\\n",
       "1  \\n\\nOf course, I won't be offended by your ans...   \n",
       "2  So that it's clear to others what was the reas...   \n",
       "\n",
       "                             llama2-13b-chat-FT-LoRA   \n",
       "0  \\n\\nYes: Explain that this text is an example ...  \\\n",
       "1  \\n\\nNote that if you do not see any implicit h...   \n",
       "2  \\n\\nHere is the short text: and i will point i...   \n",
       "\n",
       "                                           llama2-7b   \n",
       "0  \\n\\nAnswer: I do not think implicit hate speec...  \\\n",
       "1  \\n### No \\nThis text was not containing any ha...   \n",
       "2  \\n- It's clear this text is containing implici...   \n",
       "\n",
       "                                   llama2-7b-FT-LoRA   \n",
       "0  \\nThank you, I greatly appreciate any and all ...  \\\n",
       "1  \\n\\n\\n\\nHere is the text: \\n\\n\\nHey buddy what...   \n",
       "2  Thank you.\\n\\nI hope that isn't too long üëåüèºüíñ\\n...   \n",
       "\n",
       "                                        llama2-7b-FT   \n",
       "0  [/INST] You have any further or other question...  \\\n",
       "1   Can you explain? - - - - - - - - - - - - - - ...   \n",
       "2   It Is also deliberately deliberately misreali...   \n",
       "\n",
       "                                  llama2-13b-chat-FT  \n",
       "0  ,. -. a-,,\\n as of and to is the, and-. of. th...  \n",
       "1  and for the,en of ( as, is the\":, and en and o...  \n",
       "2  , that a which as,, a,,\\n./3. that the and of,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/implicit-hate-speech.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79dad217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [11:08<00:00, 222.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cuda_name = 'cuda:0'\n",
    "\n",
    "for i in tqdm(range(3)):\n",
    "#     # original model \n",
    "#     model = base_model\n",
    "#     tokenizer = llama_tokenizer\n",
    "    \n",
    "#     tweet = df.loc[0,'post']\n",
    "\n",
    "#     text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "#     inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "#     outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "#                                                pad_token_id=tokenizer.eos_token_id,\n",
    "#                                                eos_token_id=tokenizer.eos_token_id,\n",
    "#                                                return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "#     input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "#     generated_tokens = outputs.sequences[:, input_length:]\n",
    "#     temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "#     df.loc[i, 'llama2-13b-chat'] = temp_output\n",
    "    \n",
    "    # fine-tuned model, using LoRA\n",
    "    model = fine_tuning.model\n",
    "    tokenizer = llama_tokenizer\n",
    "    \n",
    "    tweet = df.loc[0,'post']\n",
    "\n",
    "    text = f\"Given the short text of: {tweet}\\nCan you answer Yes, No, or Unsure for whether this text is containing implicit hate speech? And then explain why in few setences.\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(cuda_name)\n",
    "    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1, min_length=1, do_sample=True,\n",
    "                                               pad_token_id=tokenizer.eos_token_id,\n",
    "                                               eos_token_id=tokenizer.eos_token_id,\n",
    "                                               return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    temp_output = tokenizer.decode(generated_tokens[0])\n",
    "\n",
    "    df.loc[i, 'llama2-13b-chat-FT'] = temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1e5a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/implicit-hate-speech.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c43d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
